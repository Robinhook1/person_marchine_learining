{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%writefile Tree.py\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Sat May 18 22:36:04 2019\n\nThis package defines the data structure tree and provides the fitting and testing of\nCART and SVR-Tree methods.\n\n@author: Yichen Zhu\n\"\"\"\n\nimport numpy as np\nimport collections\nimport pdb\n\n\ndef data_standardize(X):\n    ''' Function to linearly transfer feature matrix to [0,1]^d. '''\n    n, d = np.shape(X) \n    border = np.zeros((d,2))\n    for j in range(d):\n        feat_min = min(X[:,j])\n        feat_max = max(X[:,j])\n        if feat_max == feat_min:\n            raise Exception('feature '+str(j)+' has only one value')\n        border_dist = (feat_max-feat_min)/(n-1)*1\n        border[j,:] = [feat_min-border_dist, feat_max+border_dist]    \n    shifts = - border[:,0]\n    multipliers = np.diag(1/(border[:,1]-border[:,0]))\n    return np.matmul(X + np.reshape(shifts, (1,d)), multipliers)\n\n\nclass node():\n    def __init__(self):\n        pass\n\nclass tree(node):\n    def __init__(self):\n        super().__init__()\n        self.leaf = True\n        self.class_label = None\n        self.standardize_para = None\n            \n    def fit(self, X, Y, weight=1, criterion='gini', \\\n            min_split_weight=None, min_leaf_weight=None, feats_usage=None):       \n        '''\n        Function to Fit a CART.\n        \n        Parameters\n        ----------\n        X: ndarray of shape n \\times d\n            Features of data\n        Y: ndarry or list of length n\n            Response variable of data\n        weight: float\n            Weight for minority class samples. Should be no less than 1. Default value is 1.\n        criterion: 'gini'\n            Criterion for computing impurity. Currently only supports 'gini'.\n        min_split_weight: float\n            The minimal weight for a node to be further partitioned. If not provided, it will\n            be the value of parameter \"weight\".\n        min_leaf_weight: float\n            The minimal weight of lead nodes. If not provided, the program will set it to be 1.\n            \n        Returns\n        -------\n        This function does not directly return any variables. The built tree can be printed by calling\n        \"self.print()\". To predict new data with the built tree, refer to function \"predict\".        \n        '''\n        X = np.array(X)\n        Y = np.array(Y)\n        n, d = np.shape(X)\n        self.d = d\n        self.criterion = criterion\n        if min_split_weight is None:\n            self.min_split_weight = weight\n        else:\n            self.min_split_weight = min_split_weight\n        if min_leaf_weight is None:\n            self.min_leaf_weight = 1\n        else:\n            self.min_leaf_weight = min_leaf_weight\n        self.weight = weight\n        self.n, self.d = np.shape(X)           ## n: number of samples; d: number of features\n        self.wn = len(Y) + (weight-1)*sum(Y)\n        self.wy = weight*sum(Y)\n        self.impu = self.Compute_Impu(self.wy, self.wn)\n        self.impu_decr = -1\n        if feats_usage is None:\n            self.feats_usage = np.zeros(d, dtype=bool)\n        ## check stop criterion\n        if (self.wn < self.min_split_weight) or (self.wn == self.wy) or (self.wy == 0):\n            self.leaf = True\n            if self.wy/self.wn < 0.5:\n                self.class_label = 0\n            else:\n                self.class_label = 1\n            return\n        splits = np.zeros((3, self.d))  ##featureid, splitthreshold, impurity decrease\n        for i in range(self.d):\n            splits[:,i] = np.concatenate((np.array([i]), self.findsplit(X[:,i], Y)))\n        splitind = np.int_(splits[0, np.argmin(splits[2:,])])\n        self.split = splits[0:2,splitind]\n        self.impu_decr = self.impu - splits[2,splitind]\n        if self.impu_decr <= 0: \n            self.leaf = True\n            if self.wy/self.wn < 0.5:\n                self.class_label = 0\n            else:\n                self.class_label = 1\n            return            \n        else:\n            self.feats_usage[np.int_(self.split[0])] = True\n            self.leaf = False\n            leftind = np.flatnonzero(X[:,splitind]<=self.split[1])\n            self.left = tree()\n            self.left.fit(X[leftind,:], Y[leftind], weight, criterion, min_split_weight, min_leaf_weight)\n            self.right = tree()\n            self.right.fit(np.delete(X,leftind,0), np.delete(Y,leftind), weight, criterion, \\\n                           min_split_weight, min_leaf_weight)\n            self.feats_usage = np.logical_or(np.logical_or(self.feats_usage, self.left.feats_usage), self.right.feats_usage)\n            \n    def findsplit(self, x, Y):   ## x is one dimension of X, of length n\n        ''' Find the best split in one-dimensional feature x that minimizes impurity. '''\n        self.wn = len(Y) + (self.weight-1)*sum(Y)\n        self.wy = self.weight*sum(Y)\n        wyleft = 0\n        wi = 0\n        impu = 2\n        threshold = 0\n        dat = np.core.records.fromarrays(np.array([x, Y]), names='feature, label')\n        dat = np.sort(dat, order='feature')\n        for i in range(self.min_leaf_weight-1, len(Y)-self.min_leaf_weight):\n            wyleft = wyleft + self.weight*dat[i][1]\n            wi = wi + 1 + (self.weight-1)*dat[i][1]\n            if (dat[i+1][0] != dat[i][0]):\n                impu_new = self.Compute_NodeImpu(wyleft, wi, self.wy, self.wn)\n                if impu_new < impu:\n                    threshold = (dat[i+1][0]+dat[i][0]) / 2\n                    impu = impu_new\n        return np.array([threshold, impu])     \n        \n    @staticmethod\n    def Overlap_Rec(rec1, rec2):\n        upmat = rec1.copy()\n        upmat[:,0] = rec2[:,1]\n        lowmat = rec1.copy()\n        lowmat[:,1] = rec2[:,0]\n        rec = rec1.copy()\n        rec[:,0] = np.amax(lowmat, axis=1)\n        rec[:,1] = np.amin(upmat, axis=1)\n        return rec\n    \n    # @staticmethod\n    # def piecewise_fun(rec, overlap, sub_overlap, V, S, epsilon=10**(-6)):\n    #     d = np.shape(rec)[0]\n    #     ans = [None]*d\n    #     for j in range(d):\n    #         sub_overlap_j = sub_overlap[j]\n    #         slopes_changes = np.zeros((2*len(sub_overlap_j+1),2))\n    #         sidelen_j = rec[j,1] - rec[j,0]\n    #         for i in len(sub_overlap_j):\n    #             slopes_changes[i+i,:] = [sub_overlap_j[i][0], -sub_overlap_j[i][2]]\n    #             slopes_changes[i+i+1,:] = [sub_overlap_j[i][1], sub_overlap_j[i][2]]\n    #         slopes_changes = slopes_changes[np.argsort(slopes_changes[:,0]),:]\n    #         checkpoints = []\n    #         slopes = []\n    #         value = rec[j,0]\n    #         slope_all = (S - V/sidelen_j) / sidelen_j\n    #         sl = slope_all\n    #         for k in range(len(slopes_changes)): \n    #             if np.abs(slopes_changes[k,0]-value) < epsilon:\n    #                 sl += slopes_changes[k,1]\n    #             else:\n    #                 checkpoints.append(value)\n    #                 slopes.append(sl)\n    #                 if np.abs(slopes_changes[k,0]-rec[j,1]) < epsilon:\n    #                     break\n    #         slopes10 = 2*slope - slope_all\n    #         intercepts10 = np.zero(len(checkpoints)) \n    #         intercepts10[0] = overlap[]\n            \n            \n                \n        \n        \n    \n    def surface_funs(self, rec, label, reclst0, labellst0, epsilon=10**(-12)):  \n        ''' Returns all the necessary parameters to compute the change of surface of the whole\n        tree once a new partition at rec is made. Currently only working for d>=3.\n        This function concerns all surfaces bordering and inside rec.\n        '''\n        ## Processing all overlapping cells \n        d = np.shape(rec)[0]\n        V = np.prod(rec[:,1] - rec[:,0])\n        S_faces = np.zeros(d)\n        overlap = np.zeros((d, 2))   ## the overlapping surface between rec and other rectangles that are labeled 1, at two faces of feature j\n        sub_overlap = [None]*d      ## sub_overlap is a list, with each element as [start, end, sub overlapping surface]\n        for j in range(d):\n            sub_overlap[j] = []\n            S_faces[j] = V / (rec[j,1] - rec[j,0])\n        S = np.sum(S_faces) * 2\n        ans = [None]*(d+1)\n        ## If reclst is empty:\n        if len(labellst0) == 0:\n            for j in range(d):\n                intercepts10 = [2*S_faces[j]]\n                slopes10 = [(S - S_faces[j]*2) / (rec[j,1] - rec[j,0])]\n                ans[j] = ([rec[j,0]], slopes10, intercepts10, S_faces[j])  \n            ans[d] = (0, S)\n            return ans\n            \n        for i in range(len(labellst0)):\n            if labellst0[i] == 0:\n                continue\n            recnow = reclst0[i]    \n            contact_feat = -1\n            for j in range(d):\n                if rec[j,0] == recnow[j,1]:\n                    contact_feat = j\n                    contact_direct = 0\n                    break\n                elif rec[j,1] == recnow[j,0]:\n                    contact_feat = j\n                    contact_direct = 1 \n                    break\n            if contact_feat == -1:\n                continue\n            overlap_rec = self.Overlap_Rec(rec, recnow)\n            overlap_rec_del = np.delete(overlap_rec, contact_feat, axis=0)\n            if np.min(overlap_rec_del[:,1]-overlap_rec_del[:,0]) <= 0:\n                continue\n            overlap_V = np.prod(overlap_rec_del[:,1] - overlap_rec_del[:,0])\n            overlap[contact_feat, contact_direct] += overlap_V\n            feats = np.delete(np.arange(d), contact_feat)\n            for j in feats:\n                sub_overlap[j].append([overlap_rec[j,0], overlap_rec[j,1], overlap_V/(overlap_rec[j,1]-overlap_rec[j,0])])               \n        \n        ## Compute piecewise linear functions with overlapping information\n        s_0 = np.sum(overlap)\n        s_1 = S - s_0\n        ans[d] = (s_0, s_1)\n        for j in range(d):\n            sub_overlap_j = sub_overlap[j]\n            if len(sub_overlap_j) == 0:\n                intercepts10 = [s_0 - overlap[j,0] + S_faces[j] - overlap[j,0] + S_faces[j]]\n                slopes10 = [(S - S_faces[j]*2) / (rec[j,1] - rec[j,0])]\n                ans[j] = ([rec[j,0]], slopes10, intercepts10, S_faces[j])   \n                continue\n            slopes_changes = np.zeros((2*len(sub_overlap_j),2))  ## both slopes_changes and slopes depicts slopes overlapping with elements of reclst with label 1\n            sidelen_j = rec[j,1] - rec[j,0]\n            for i in range(len(sub_overlap_j)):\n                slopes_changes[i+i,:] = [sub_overlap_j[i][0], sub_overlap_j[i][2]]\n                slopes_changes[i+i+1,:] = [sub_overlap_j[i][1], -sub_overlap_j[i][2]]\n            slopes_changes = slopes_changes[np.argsort(slopes_changes[:,0]),:]\n            checkpoints = []\n            slopes = []\n            value = rec[j,0]\n            slope_all = (S - S_faces[j]*2) / sidelen_j\n            sl = 0\n            for k in range(len(slopes_changes)): \n                if np.abs(slopes_changes[k,0]-value) < epsilon:\n                    sl += slopes_changes[k,1]\n                else:\n                    checkpoints.append(value)\n                    value = slopes_changes[k,0]\n                    slopes.append(sl)\n                    sl += slopes_changes[k,1]\n                    if np.abs(slopes_changes[k,0]-rec[j,1]) < epsilon:\n                        break\n            try:                \n                if len(checkpoints) == 0:\n                    intercepts10 = [s_0 - overlap[j,0] + S_faces[j] - overlap[j,0] + S_faces[j]]\n                    slopes10 = [(S - S_faces[j]*2) / (rec[j,1] - rec[j,0])]\n                    ans[j] = ([rec[j,0]], slopes10, intercepts10, S_faces[j])   \n                    continue\n                if np.abs(checkpoints[-1]-value) >= epsilon:\n                    checkpoints.append(value)\n                    slopes.append(sl)\n            except:\n                pdb.set_trace()\n                debug = checkpoints\n            slopes10 = slope_all - 2*np.array(slopes)\n            intercepts10 = np.zeros(len(checkpoints)) \n            intercepts10[0] = s_0 - overlap[j,0] + S_faces[j] - overlap[j,0] + S_faces[j]\n            for k in range(1,len(checkpoints)):\n                if checkpoints[k] < checkpoints[k-1]:\n                    print('Error: invalid checkpoints: '+str(checkpoints))\n                intercepts10[k] = intercepts10[k-1] + slopes10[k-1]*(checkpoints[k]-checkpoints[k-1])\n            ans[j] = (checkpoints, slopes10, intercepts10, S_faces[j])            \n        return ans            \n                \n    def fit_sv(self, X, Y, pen, feature_select=False, c0=1, weight=1, border=None, standardize=False, \n               criterion='gini', min_split_weight=None, min_leaf_weight=None, tol=10**(-10), maximal_leaves=None):       \n        '''\n        Function to Fit a SVR-Tree.\n        \n        Parameters\n        ----------\n        X: ndarray of shape n \\times d\n            Features of data\n        Y: ndarry or list of length n\n            Response variable of data\n        pen: float\n            Penalty parameter of surface-to-volume ratio. We suggest to try values in the\n            interval [0.001, 1]\\times n^{-1/3}.\n        feature_select: boolean\n            Whether feature selection steps are enabled. Default value is False.\n        c0: float\n            c0 parameter is feature selections. If feature_select=False, c0 does not have\n            any impacts on this function. Default value is 1.\n        weight: float\n            Weight for minority class samples. Should be no less than 1. Default value is 1.\n        border: ndarray of shape d \\times 2\n            A hyperrectangle where the features of data lie in. If not provided, the program \n            will automatically compute one. If \"border\" is provided, \"standardized\" must by True.\n        standardize: boolean\n            Whether the features are already standardized. By saying standardized, it means the\n            data is already transformed to lie in \"border\". It is recommanded that users do not \n            mannually input values for both \"border\" and \"standardize\", in which case the program\n            will automatically pre-process the dataset.        \n        criterion: 'gini'\n            Criterion for computing impurity. Currently only supports 'gini'.\n        min_split_weight: float\n            The minimal weight for a node to be further partitioned. If not provided, it will\n            be the value of parameter \"weight\".\n        min_leaf_weight: float\n            The minimal weight of lead nodes. If not provided, the program will set it to be 1.\n        tol: float\n            Tolerance for errors in comparison. Default is 10^(-5).\n        maximal_leaves:\n            Maximal number of leave nodes. If not provided, the program will run until no partition\n            can be further accepted.\n            \n        Returns\n        -------\n        This function does not directly return any variables. The built tree can be printed by calling\n        \"self.print()\". To predict new data with the built tree, refer to function \"predict\".        \n        '''\n        X = np.array(X)\n        Y = np.array(Y)\n        n, d = np.shape(X)           ## n: number of samples; d: number of features\n        self.d = d\n        if border == None:\n            border = np.zeros((d,2))\n            border[:,1] = 1\n        if not standardize:\n            X = self.data_standardize(X)\n        if min_split_weight == None:\n            min_split_weight = weight+1\n        if min_leaf_weight == None:\n            min_leaf_weight = 1\n        if maximal_leaves == None:\n            maximal_leaves = np.floor(np.sqrt(n))\n        wn_all = len(Y) + (weight-1)*sum(Y)\n        self.wn = wn_all\n        self.wy = weight*sum(Y)\n        self.impu = self.Compute_Impu(self.wy, self.wn)\n        self.class_label = 1\n        self.sign_impu = self.Compute_SignImpu(self.wy, self.wn, self.class_label)\n        tree_impu = self.impu\n        tree_sign_impu = self.sign_impu\n\n        volume = np.prod(border[:,1] - border[:,0])\n        surface = 0\n        for j in range(d):\n            surface += 2 * volume / (border[j,1]-border[j,0])\n        sv_reg_min = self.sv_regular(surface, volume, d)\n        risk = tree_impu + pen * sv_reg_min\n        self.class_label = int(self.wy/self.wn>=0.5)\n        self.rec = border\n        self.X = X\n        self.Y = Y\n        node_que = collections.deque([self])   ## node_que is the queue that stores the nodes to operate, right side in and left side out\n        rec_que = collections.deque([border]) \n        label_que = collections.deque([1])        \n        reclst_leg = []\n        labellst_leg = []\n        feats_usage = np.zeros(d, dtype=bool)\n        n_operate_nodes = 1\n\n        while len(node_que) > 0 and n_operate_nodes < maximal_leaves:    ## Note surface, volume, tree_impu are attributes of a certain subtree (which contains root) rather than a node\n            n_operate_nodes += 1\n            node = node_que.popleft()\n            rec = rec_que.popleft()\n            reclst = list(rec_que)\n            reclst.extend(reclst_leg)\n            label = label_que.popleft()\n            labellst = list(label_que)\n            labellst.extend(labellst_leg)\n            ans = self.surface_funs(rec, label, reclst, labellst)    ## ans contains information about changes of surface after partitions\n            s_0, s_1 = ans[d]\n            if label == 1:\n                s_origin = s_1\n            else:\n                s_origin = s_0\n            volume0 = volume - label * np.prod(rec[:,1] - rec[:,0])  ## The quantities subtitled by 0 remain unchanged through the next for loop\n            # print(volume0, rec)\n            if volume0 < -tol:          ## a bug-checking procedure\n                pdb.set_trace()\n                print('Negative volume0: '+str(volume0))\n                raise Exception('Negative volume0: '+str(volume0))\n            surface0 = surface\n            tree_impu0 = tree_impu - node.impu * node.wn\n            tree_sign_impu0 = tree_sign_impu - node.sign_impu * node.wn/wn_all\n            \n            featureid = -1         ## featureid=-1 means no better partition is found\n            feats_reorder = np.append(np.flatnonzero(feats_usage), np.flatnonzero(1-feats_usage))\n            node_impu_selected = node.impu\n            S_faces = np.zeros(d)\n            for j in feats_reorder:\n                checkpoints, slope10, intercept10, S_faces[j] = ans[j]\n                loc = 0          ## loc is the largest index of checkpoints that are no greater than thre\n                wleft = 0\n                wyleft = 0\n                dat = np.core.records.fromarrays(np.array([node.X[:,j], node.Y]), names='feature, label')\n                dat = np.sort(dat, order='feature')\n                for sa in range(len(node.Y)-1):    ## sa is short for sample                        \n                    wyleft = wyleft + weight*dat[sa][1]\n                    wleft = wleft + 1 + (weight-1)*dat[sa][1]\n                    try:\n                        if wleft < min_leaf_weight or dat[sa][0]-rec[j,0] < tol:\n                            pass\n                        elif node.wn - wleft < min_leaf_weight or rec[j,1]-dat[sa+1][0]< tol:\n                            pass\n                    except:\n                        pdb.set_trace()\n                        print(dat[sa][0], dat[sa+1][0], rec[j,0], rec[j,1])\n                    if wleft < min_leaf_weight or dat[sa][0]-rec[j,0] < tol:\n                        continue\n                    elif node.wn - wleft < min_leaf_weight or rec[j,1]-dat[sa+1][0]< tol:\n                        break\n                    if (dat[sa+1][0] != dat[sa][0]):\n                        thre_new = (dat[sa+1][0]+dat[sa][0]) / 2\n                        node_impu_new = self.Compute_NodeImpu(wyleft, wleft, node.wy, node.wn)\n                        while loc < len(checkpoints)-1 and checkpoints[loc+1] <= thre_new:\n                            loc += 1\n                            \n                        if feature_select:\n                            if feats_usage[j]:\n                                node_impu_selected = min(node_impu_selected, node_impu_new)\n                            else:\n                                if node_impu_selected-node_impu_new < c0*pen*wn_all/node.wn:\n                                    continue\n                        tree_impu_new = node_impu_new * node.wn / wn_all + tree_impu0\n\n                        \n                        tree_sign_impu_new_lst = [tree_sign_impu0]*4\n                        surface_new_lst = [0,0,0,0]\n                        volume_new_lst = [0,0,0,0]\n                        risk_new_lst = [0,0,0,0]\n                        child_labels_lst = [[1,1], [0,0], [0,1], [1,0]]\n                        \n                        '''If both child nodes are labeled 1'''\n                        surface_new_lst[0] = surface0 + s_1 - s_origin\n                        volume_new_lst[0] = np.prod(rec[:,1] - rec[:,0]) + volume0\n                        tree_sign_impu_new_lst[0] = tree_sign_impu_new_lst[0] + node.wn / wn_all * self.Compute_SignNodeImpu(wyleft, wleft, node.wy, node.wn, [1,1])\n                        if volume_new_lst[0] <= 0 or surface_new_lst[0] <= 0:\n                            svr = sv_reg_min\n                        else:\n                            svr = self.sv_regular(surface_new_lst[0], volume_new_lst[0], d) \n                        risk_new_lst[0] = tree_sign_impu_new_lst[0] + pen*svr \n                        \n                        '''If both child nodes are labeled 0'''\n                        surface_new_lst[1] = surface0 + s_0 - s_origin\n                        volume_new_lst[1] = volume0\n                        tree_sign_impu_new_lst[1] = tree_sign_impu_new_lst[1] + node.wn / wn_all * self.Compute_SignNodeImpu(wyleft, wleft, node.wy, node.wn, [0,0])\n                        if volume_new_lst[1] <= 0 or surface_new_lst[1] <= 0:\n                            svr = sv_reg_min\n                        else:\n                            svr = self.sv_regular(surface_new_lst[1], volume_new_lst[1], d) \n                        risk_new_lst[1] = tree_sign_impu_new_lst[1] + pen*svr                    \n\n                        '''If left child is labeled 0 and right child is labeled 1'''\n                        surface_new_lst[2] = surface0 + s_0 + s_1 + 2*S_faces[j] - (intercept10[loc] + slope10[loc]*(thre_new-checkpoints[loc])) - s_origin\n                        volume_new_lst[2] = volume0 + np.prod(np.delete(rec[:,1],j)-np.delete(rec[:,0],j)) * (rec[j,1]-thre_new)\n                        tree_sign_impu_new_lst[2] = tree_sign_impu_new_lst[2] + node.wn / wn_all * self.Compute_SignNodeImpu(wyleft, wleft, node.wy, node.wn, [0,1])\n                        if volume_new_lst[2] <= 0 or surface_new_lst[2] <= 0:\n                            svr = sv_reg_min\n                        else:\n                            svr = self.sv_regular(surface_new_lst[2], volume_new_lst[2], d)\n                        risk_new_lst[2] = tree_sign_impu_new_lst[2] + pen*svr \n\n                        '''If left child is labeled 1 and right child is labeled 0'''\n                        surface_new_lst[3] = surface0 + intercept10[loc] + slope10[loc]*(thre_new-checkpoints[loc]) - s_origin\n                        volume_new_lst[3] = volume0 + np.prod(np.delete(rec[:,1],j)-np.delete(rec[:,0],j)) * (thre_new-rec[j,0])\n                        tree_sign_impu_new_lst[3] = tree_sign_impu_new_lst[3] + node.wn / wn_all * self.Compute_SignNodeImpu(wyleft, wleft, node.wy, node.wn, [1,0])\n                        if volume_new_lst[3] <= 0 or surface_new_lst[3] <= 0:\n                            svr = sv_reg_min\n                        else:\n                            svr = self.sv_regular(surface_new_lst[3], volume_new_lst[3], d)\n                        risk_new_lst[3] = tree_sign_impu_new_lst[3] + pen*svr    \n                        \n                        argmin = np.argmin(risk_new_lst)\n                        \n                        if np.min(surface_new_lst) < - tol:\n                            print('reclst:', reclst)\n                            print('rec:', rec, 'len(reslst):', len(reclst))                            \n                            print('slope10:', slope10, 'intercept10:', intercept10)\n                            print('Negative surface: '+str(np.min(surface_new_lst))+'  pen: '+str(pen)+'  type: '+str(np.argmin(surface_new_lst)))\n                            print('volume0:', volume0, 'surface0:', surface0)\n                            print('featureid_now:', j, 'thre_now:', thre_new)\n                            \n                            pdb.set_trace()\n                            raise Exception('Negative surface: '+str(np.min(surface_new_lst))+'  pen:'+str(pen))\n                        if np.min(tree_sign_impu_new_lst) < - tol:\n                            print('Negative tree signed impurity: '+str(np.min(tree_sign_impu_new_lst)))\n\n                        if risk_new_lst[argmin] < risk:                           \n                            thre = thre_new\n                            featureid = j\n                            child_labels = child_labels_lst[argmin]\n                            surface = surface_new_lst[argmin]\n                            volume = volume_new_lst[argmin]\n                            tree_impu = tree_impu_new   \n                            tree_sign_impu = tree_sign_impu_new_lst[argmin]\n                            risk = risk_new_lst[argmin]\n                            if risk < -tol:\n                                print('Negative risk: '+str(risk)+'  pen:'+str(pen))\n                                print('signed impu: '+str(tree_sign_impu))\n                                print('volume: '+str(volume))\n                                print('surface: '+str(surface))\n                                pdb.set_trace()\n                                raise Exception('Negative risk: '+str(risk)+'  pen:'+str(pen))\n\n            if featureid >= 0:                     ## i.e., a better partition is found\n                node.leaf = False\n                feats_usage[featureid] = True\n                node.split = [featureid, thre]\n                node.left = tree()\n                node.left.standardize_para = node.standardize_para\n                leftind = np.flatnonzero(node.X[:,featureid]<=thre)\n                node.left.X = node.X[leftind,]\n                node.left.Y = node.Y[leftind]\n                node.left.wn = len(node.left.Y) + (weight-1) * sum(node.left.Y)\n                node.left.wy = weight * sum(node.left.Y)\n                node.left.impu = self.Compute_Impu(node.left.wy, node.left.wn)\n                node.left.class_label = child_labels[0]\n                node.left.sign_impu = self.Compute_SignImpu(node.left.wy, node.left.wn, node.left.class_label)\n                node.left.rec = rec.copy()\n                node.left.rec[featureid,1] = thre\n                if node.left.wy == 0 or node.left.wy == node.left.wn or node.left.wn < min_split_weight:\n                    node.left.leaf = True\n                    if node.left.class_label == 1:\n                        reclst_leg.append(node.left.rec)\n                        labellst_leg.append(1)\n                else:\n                    node_que.append(node.left)\n                    rec_que.append(node.left.rec)\n                    label_que.append(node.left.class_label)\n                node.right = tree()\n                node.right.standardize_para = node.standardize_para\n                rightind = np.flatnonzero(node.X[:,featureid]>thre)\n                node.right.X = node.X[rightind,]\n                node.right.Y = node.Y[rightind]\n                node.right.wn = len(node.right.Y) + (weight-1) * sum(node.right.Y)\n                node.right.wy = weight * sum(node.right.Y)\n                node.right.impu = self.Compute_Impu(node.right.wy, node.right.wn)\n                node.right.class_label = child_labels[1]\n                node.right.sign_impu = self.Compute_SignImpu(node.right.wy, node.right.wn, node.right.class_label)\n                node.right.rec = rec.copy()\n                node.right.rec[featureid,0] = thre\n                node.right.rec[featureid,0] = thre\n                if node.right.wy == 0 or node.right.wy == node.right.wn or node.right.wn < min_split_weight:\n                    node.right.leaf = True\n                    if node.right.class_label == 1:\n                        reclst_leg.append(node.right.rec)\n                        labellst_leg.append(1)\n                else:\n                    node_que.append(node.right)\n                    rec_que.append(node.right.rec)\n                    label_que.append(node.right.class_label)\n            else:\n                if node.class_label == 1:\n                    reclst_leg.append(node.rec)\n                    labellst_leg.append(1)\n                    \n        self.feats_usage = feats_usage\n        return\n\n\n    def data_standardize(self, X):\n        ''' A function of class tree which linearly transfers feature matrix to [0,1]^d. '''\n        n, d = np.shape(X) \n        border = np.zeros((d,2))\n        for j in range(d):\n            feat_min = min(X[:,j])\n            feat_max = max(X[:,j])\n            if feat_max == feat_min:\n                raise Exception('feature '+str(j)+' has only one value')\n            border_dist = (feat_max-feat_min)/(n-1)\n            border[j,:] = [feat_min-border_dist, feat_max+border_dist]    \n        shifts = - border[:,0]\n        multipliers = np.diag(1/(border[:,1]-border[:,0]))\n        self.standardize_para = (shifts, multipliers)\n        return np.matmul(X + np.reshape(shifts, (1,d)), multipliers)  \n                    \n    @staticmethod\n    def sv_regular(surface, volume, d):\n        ''' Compute surface-to-volume regularization. '''\n        return surface/volume\n    \n    @staticmethod\n    def Compute_Impu(wy, w, criterion='gini'):\n        ''' Compute impurity of a node. '''\n        return 1 - (wy/w)**2 - ((w-wy)/w)**2 \n        \n    @staticmethod\n    def Compute_SignImpu(wy, w, label, criterion='gini'):\n        ''' Compute signed impurity of a node. '''\n        if int(wy/w>=0.5) == label:\n            return 1 - (wy/w)**2 - ((w-wy)/w)**2\n        else:\n            return (wy/w)**2 + ((w-wy)/w)**2\n        \n    @staticmethod\n    def Compute_NodeImpu(wyleft, wleft, wy, w, criterion='gini'):\n        ''' Compute impurity of a node after a partition. '''\n        return 1 - ((wyleft/wleft)**2 + ((wleft-wyleft)/wleft)**2)*wleft/w \\\n                - (((wy-wyleft)/(w-wleft))**2 + ((w-wleft-wy+wyleft)/(w-wleft))**2)*(w-wleft)/w \n    \n    @staticmethod\n    def Compute_SignNodeImpu(wyleft, wleft, wy, w, child_labels, criterion='gini'):\n        ''' Compute signed impurity of a node after a partition. '''\n        impu_left = 1 - (wyleft/wleft)**2 - ((wleft-wyleft)/wleft)**2\n        impu_right = 1 - ((wy-wyleft)/(w-wleft))**2 - ((w-wleft-wy+wyleft)/(w-wleft))**2\n        if int(wyleft/wleft>=0.5) == child_labels[0]:\n            impu_left_sign = impu_left\n        else:\n            impu_left_sign = 1 - impu_left\n        if int((wy-wyleft)/(w-wleft)>=0.5) == child_labels[1]:\n            impu_right_sign = impu_right\n        else:\n            impu_right_sign = 1 - impu_right\n        return impu_left_sign*wleft/w + impu_right_sign*(w-wleft)/w  \n    \n                \n    def predict(self, X):    \n        '''\n        This function return predict class labels for a new data using the tree \"self\".\n        \n        Parameters\n        ----------\n        X: ndarray\n            Feature matrix of new data. Must has the same number of features as \n            the training data.\n        \n        Returns\n        -------\n        var: ndarray\n            One-dimensional array contains the predicted class labels of new data.\n        '''\n        X = np.array(X)\n        d = np.shape(X)[1]\n        if not self.standardize_para == None:\n            shifts, multipliers = self.standardize_para\n            X = np.matmul(X + np.reshape(shifts, (1,d)), multipliers)\n        return self.localpredict(X)\n    \n    def localpredict(self, X): \n        ''' This recursive functions is called by function \"predict\" to complete \n        its taks of predicting class labels. '''\n        if self.leaf:\n            return self.class_label * np.ones(np.shape(X)[0],dtype=int)\n        else:\n            Y = np.zeros(np.shape(X)[0],dtype=int)\n            featureid, thre = self.split\n            featureid = np.int_(featureid)\n            leftind = np.flatnonzero(X[:,featureid]<=thre)\n            Y[leftind] = self.left.localpredict(X[leftind,:])\n            rightind = np.flatnonzero(X[:,featureid]>thre)\n            Y[rightind] = self.right.localpredict(X[rightind,:])\n            return Y\n            \n    def compute_feats_usage(self):\n        '''\n        This function checks whether each feature is used in partitions. It does \n        not take any input parameters except \"self\".\n        \n        Returns\n        -------\n        var: ndarray\n            One-dimensional array of length equal to the number of features used\n            for training. The ith element of this array is True if the ith feature\n            is used for partitions and False otherwise.\n        '''\n        feats_usage = np.zeros(self.d, dtype=bool)\n        self.feats_usage = self.local_feats_usage(feats_usage) \n    \n    def local_feats_usage(self, feats_usage):\n        ''' This recursive is called by \"compute_feats_usage\" to check whether each\n        feature is used for partitions. '''\n        if self.leaf:\n            return feats_usage\n        else:\n            featureid = np.int_(self.split[0])\n            feats_usage[featureid] = True\n            feats_usage = self.left.local_feats_usage(feats_usage)\n            feats_usage = self.right.local_feats_usage(feats_usage)\n            return feats_usage\n    \n    def print(self, init=True, print_weight=False, print_impu=False):\n        '''\n        This function print a tree.\n        \n        Parameters\n        ----------\n        init: boolean\n            Whether the printing is started from root node. If not called by the \n            the function \"print\" itself, it should always set to be True. Default \n            value is True.\n        print_weight: boolean\n            Whether to print the weight of training samples in each node. Default\n            is False.\n        print_impu: boolean\n            Whether to print the impurity of training samples in each node. Default\n            is False.\n        \n        Returns\n        -------\n        This function returns nothing.\n        \n        Outputs\n        -------\n        This function will print all the nodes of the tree in a depth-first order.\n        '''\n        if init:\n            self.codename = 'root'\n        if self.leaf:\n            print(self.codename+':', self.class_label)\n            if print_weight:\n                print('class 1 weight, total weight:', self.wy, self.wn)\n            if print_impu:\n                print('impurity:', self.impu)                \n        else:\n            print(self.codename+':', 'feature '+str(self.split[0])+' <= '+str(self.split[1]))\n            if print_weight:\n                print('class 1 weight and total weight:', self.wy, self.wn)\n            if print_impu:\n                print('impurity, impurity_decr:', self.impu, self.impu_decr, self.tot_impudecr, self.alpha)\n            self.left.codename = self.codename + '.left'\n            self.left.print(False, print_weight, print_impu)\n            self.right.codename = self.codename + '.right'\n            self.right.print(False, print_weight, print_impu)\n         \n            \n    def copy(self):\n        ''' Copy the current tree represented by \"self\". '''\n        copytr = tree()\n        copytr.leaf = self.leaf\n        copytr.impu = self.impu\n        copytr.wn = self.wn\n        copytr.wy = self.wy\n        if self.leaf:\n            copytr.class_label = self.class_label\n        else:\n            copytr.split = self.split\n            copytr.impu_decr = self.impu_decr\n            copytr.left = self.left.copy()\n            copytr.right = self.right.copy()\n        return copytr\n        \n    def Find_prune(self, totalweight, alpha_min=np.inf, prunetree=[]):\n        ''' This function called by \"prune\" to complete its task of pruning a tree. '''\n        if self.leaf:\n            self.tot_impudecr = 0\n            self.tot_leaf = 1\n            return (alpha_min, prunetree)\n        else:\n            alpha_min, prunetree = self.left.Find_prune(totalweight, alpha_min, prunetree)\n            alpha_min, prunetree = self.right.Find_prune(totalweight, alpha_min, prunetree)\n            left_prop = self.left.wn / self.wn\n            right_prop = self.right.wn / self.wn\n            self.tot_impudecr = self.left.tot_impudecr * left_prop + self.right.tot_impudecr * right_prop \\\n                                    + self.impu_decr\n            self.tot_leaf = self.left.tot_leaf + self.right.tot_leaf\n            self.alpha = self.tot_impudecr * self.wn / (totalweight * (self.tot_leaf-1))\n            if self.alpha < alpha_min:\n                alpha_min = self.alpha\n                prunetree = [self]\n            elif self.alpha == alpha_min:\n                prunetree.append(self)\n            return (alpha_min, prunetree)\n    \n    def Prune(self):\n        '''\n        This function prunes a CART. Users should not use this function to prune\n        a SVR-Tree, despite doing that does not yield an error in codes.\n        \n        Parameters\n        ----------\n        This function takes no parameters except \"self\".\n        \n        Returns\n        -------\n        var=(treelst, alphalst), tot_leaf_lst): tuple of length 3. each element\n        is explained as below:\n            treelst: a list of possible optimal CART tree after pruning.\n            alphalst: an unidimensional ndarry of the same length as \"treelst\". Each\n                element represents the alpha value when the corresponding element \n                in \"treelst\" is the optimally pruned CART tree.\n            tot_leaf_lst: a list of the same length as \"treelst\". Each element contains\n                the number of leaf nodes of the corresponding element in \"treelst\".\n        '''\n        treelst = [self]\n        alphalst = [0]\n        tot_leaf_lst = []\n        tr = self\n        while tr.leaf == False:\n            tr_next = tr.copy()\n            tr_next.d = self.d\n            alpha, prunetree = tr_next.Find_prune(totalweight=self.wn)\n            alphalst.append(alpha)\n            tot_leaf_lst.append(tr_next.tot_leaf)       ## The total leaf value lags one iteration \n            for i in prunetree:\n                i.leaf = True\n                i.left = None\n                i.right = None\n                i.class_label = int(i.wy/i.wn >= 0.5)\n            treelst.append(tr_next)\n            tr = tr_next\n        tot_leaf_lst.append(1)                ## a tree with only root node has total leaf 1\n        return (treelst, np.array(alphalst), tot_leaf_lst)\n\n        \n        \n        \n\n            \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n    \n\n        \n","metadata":{"execution":{"iopub.status.busy":"2022-01-04T05:42:55.394314Z","iopub.execute_input":"2022-01-04T05:42:55.394645Z","iopub.status.idle":"2022-01-04T05:42:55.426911Z","shell.execute_reply.started":"2022-01-04T05:42:55.394615Z","shell.execute_reply":"2022-01-04T05:42:55.425629Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"%%writefile sampler.py\n\"\"\"\nCreated on Thu Jun 20 15:04:47 2019\n\nThis package perform oversampling on datasets. Supported methods are:\nDuplicated oversampling, SMOTE, Borderline-SMOTE, ADASYN.\n\n@author: Yichen Zhu\n\"\"\"\nimport numpy as np\n\nclass sampler():\n    def __init__(self, times=1, method='duplicate', mBSMOTE=10):\n        '''\n        Initiation function for sampler class.\n        \n        Parameters\n        ----------\n        times: integer\n            Number of times the original data will be resampled. Default value is 1.\n        method: 'duplicate' or 'SMOTE' or 'BSMOTE' or 'ADASYN'.\n            Which method is used for resampling. Default value is 'duplicate'.        \n        '''\n        self.times = int(times)\n        self.method = method\n        self.mBSMOTE = mBSMOTE\n    \n    @staticmethod\n    def neighbor_ind(X, s):\n        '''Find the s-nearest neighbors of each sample, return with a n\\times s \n        matrix containing indices of nearest neighbors.\n        '''\n        n = np.shape(X)[0]\n        Dmat = np.zeros((n, n))\n        for i in range(n-1):\n            for j in range(i+1,n):\n                Dmat[i,j] = np.dot(X[i,:]-X[j,:], X[i,:]-X[j,:])\n                Dmat[j,i] = Dmat[i,j]\n        E = np.zeros((n, s), dtype=int)\n        for i in range(n):\n            di = np.core.records.fromarrays(np.array([Dmat[i,:], np.array(range(n))]), names='distance, index')\n            di = np.sort(di, order='distance')  \n            E[i,:] = di['index'][0:s]\n        return E  \n    \n    @staticmethod\n    def neighbor_ind_half(X, Xplus, s):\n        '''Find the s-nearest neighbors from Xplus for each sample in X, return \n        with a n\\times s matrix containing indices of nearest neighbors.\n        '''\n        n1 = np.shape(X)[0]\n        n = np.shape(Xplus)[0]\n        Dmat = np.zeros((n1, n))\n        for i in range(n1):\n            for j in range(n):\n                Dmat[i,j] = np.dot(X[i,:]-Xplus[j,:], X[i,:]-Xplus[j,:])\n        E = np.zeros((n1, s), dtype=int)\n        for i in range(n1):\n            di = np.core.records.fromarrays(np.array([Dmat[i,:], np.array(range(n))]), names='distance, index')\n            di = np.sort(di, order='distance')  \n            E[i,:] = di['index'][0:s]\n        return E  \n        \n    def fit_resample(self, X, Y):\n        '''\n        Main function to return resampled data. For how many times the dataset will be \n        resampled and which method is used for resampled, see parameters of initiation\n        function.\n        \n        Parameters\n        ----------\n        X: ndarray of shape n \\times d\n            The features of orignial data.\n        Y: ndarry or list of length n\n            The outcome variable of original data.\n            \n        Returns\n        -------\n        val: ndarray\n            The features of data after oversampling.\n        '''\n        if self.times <= 0:\n            return (X, Y)\n        n, d = np.shape(X)\n        c1_ind = np.flatnonzero(Y)\n        n1 = len(c1_ind)\n        X1 = X[c1_ind, :]\n        if self.method == 'duplicate':\n            X1_res = np.zeros((n1*self.times, d))\n            for ti in np.arange(self.times):\n                X1_res[(ti*n1):((ti+1)*n1),:] = X1.copy()\n            X_res = np.vstack((X, X1_res))\n            Y_res = np.append(Y, np.ones(n1*self.times))\n        elif self.method == 'SMOTE':\n            if self.times <= 5:\n                E = self.neighbor_ind(X1, 5)    \n                X1_res = np.zeros((n1*self.times, d))\n                for i in range(n1):\n                    X_ne = X1[E[i,np.random.choice(5,self.times,replace=False)],:]\n                    unif_r = np.random.random(self.times)\n                    X_syn = np.dot(np.diag(unif_r), X_ne-X1[i,:]) + X1[i,:]\n                    X1_res[(i*self.times):((i+1)*self.times),:] = X_syn\n            else:\n                E = self.neighbor_ind(X1, 5)    \n                X1_res = np.zeros((n1*self.times, d))\n                for i in range(n1):\n                    X_ne = X1[E[i,np.random.choice(5,self.times,replace=True)],:]\n                    unif_r = np.random.random(self.times)\n                    X_syn = np.dot(np.diag(unif_r), X_ne-X1[i,:]) + X1[i,:]\n                    X1_res[(i*self.times):((i+1)*self.times),:] = X_syn\n            X_res = np.vstack((X, X1_res))\n            Y_res = np.append(Y, np.ones(n1*self.times))\n        elif self.method == 'BSMOTE':\n            m = self.mBSMOTE\n            Eall = self.neighbor_ind_half(X1, X, m)\n            minor_num = np.zeros(n1)\n            for i in range(n1):\n                minor_num[i] = int(sum(Y[Eall[i,:]]))\n            ss = 0\n            for j in range(m):\n                ss = ss + len(np.flatnonzero(minor_num == j))\n                if ss >= n1 / 2:\n                    break\n            c1_border_ind = np.flatnonzero(minor_num <= j)\n            n1_border = len(c1_border_ind)\n            X1_border = X1[c1_border_ind]\n            if self.times <= 5:\n                E = self.neighbor_ind_half(X1_border, X1, 5)\n                X1_res = np.zeros((n1_border*self.times, d))\n                for i in range(n1_border):\n                    X_ne = X1[E[i,np.random.choice(5,self.times,replace=False)],:]\n                    x = X1_border[i,:]\n                    unif_r = np.random.random(self.times)\n                    X_syn = np.dot(np.diag(unif_r), X_ne-x) + x\n                    X1_res[(i*self.times):((i+1)*self.times),:] = X_syn \n            else:\n                E = self.neighbor_ind_half(X1_border, X1, 5)\n                X1_res = np.zeros((n1_border*self.times, d))\n                for i in range(n1_border):\n                    X_ne = X1[E[i,np.random.choice(5,self.times,replace=True)],:]\n                    x = X1_border[i,:]\n                    unif_r = np.random.random(self.times)\n                    X_syn = np.dot(np.diag(unif_r), X_ne-x) + x\n                    X1_res[(i*self.times):((i+1)*self.times),:] = X_syn                \n            X_res = np.vstack((X, X1_res))\n            Y_res = np.append(Y, np.ones(n1_border*self.times))\n        elif self.method == 'ADASYN':\n            m = 5\n            Eall = self.neighbor_ind_half(X1, X, m)\n            major_rate = np.zeros(n1)\n            for i in range(n1):\n                major_rate[i] = 1 - sum(Y[Eall[i,:]])/m\n            if np.sum(major_rate) == 0:\n                major_rate = np.ones(n1)\n            else:\n                major_rate = major_rate / np.average(major_rate)\n            n1res_lst = np.rint(major_rate * self.times)\n            n1res_lst = n1res_lst.astype(int)\n            n1_res = sum(n1res_lst)\n            X1_res = np.zeros((n1_res, d))\n            s = 5\n            E = self.neighbor_ind(X1, s)\n            isum = 0\n            for i in range(n1):\n                if n1res_lst[i] >= 1:\n                    X_ne = X1[E[i,np.random.choice(5,n1res_lst[i],replace=True)],:]\n                    unif_r = np.random.random(n1res_lst[i])\n                    X_syn = np.dot(np.diag(unif_r), X_ne-X1[i,:]) + X1[i,:]\n                    X1_res[isum:(isum+n1res_lst[i]),:] = X_syn \n                    isum = isum + n1res_lst[i]\n            X_res = np.vstack((X, X1_res))\n            Y_res = np.append(Y, np.ones(n1_res))\n        else:\n            raise ValueError('There is no such a method.')\n            \n        return (X_res, Y_res)\n    \n              \n  ","metadata":{"execution":{"iopub.status.busy":"2022-01-04T05:45:03.733549Z","iopub.execute_input":"2022-01-04T05:45:03.733870Z","iopub.status.idle":"2022-01-04T05:45:03.744629Z","shell.execute_reply.started":"2022-01-04T05:45:03.733838Z","shell.execute_reply":"2022-01-04T05:45:03.743667Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Thu Feb 20 17:09:08 2020\n\nThis file is used to test the performance of SVR-Tree and other Tree based imbalanced\nclassification methods. \n\n@author: acezy\n\"\"\"\n\n'''to execute inside python, type: exec(open('test_data_nested_cv_linux.py').read())'''\n\n\nimport sys\n''' Please append your own path here. '''\n'''sys.path.append('D:/Research/AUC_IB/code2020')\nsys.path.append('D:/Research/AUC_IB/data')\nsys.path.append('/home/grad/yz486/ImbalanceData/code2021')\nsys.path.append('/home/grad/yz486/ImbalanceData/data')'''\nimport numpy as np\nimport pandas as pd\nimport Tree\nimport multiprocessing as multip\nimport time\nimport sampler\n\n\n\nt0 = time.time()\n\n''' Reading data sets. Please choose data_name from: \n    'vechicle', 'pima', 'abalone', 'satimage', 'wine', 'glass', 'page', 'yeast', 'shuttle', 'segment', 'vowel', 'ecoli', 'penbased'  '''\n   \ndata_name = 'titanic'\n\nif data_name == 'vehicle':\n    '''vehicle data'''\n    da = [None] * 9\n    filenames = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i']\n    for i in range(9):\n        da[i] = pd.read_table('../input/imbalance-person/Classification-Tree-with-Surface-to-Volume-ratio-Regularization-master/xa'+filenames[i]+'.dat', sep=' ',\n                              header=None, error_bad_lines=False,keep_default_na=False)    ## read file in linux\n    da_vehicle = pd.concat(da)\n    da_vehicle['class'] = np.array(da_vehicle[18] == 'van', dtype=int)\n    da_vehicle = da_vehicle.drop([18], axis=1)\n    da = da_vehicle.values\n    Xall = da[:,0:18]\n    Xall = Xall.astype('float')\n    for i in range(18):\n        Xall[:,i] = Xall[:,i] / np.max(Xall[:,i])\n    Yall = da[:,18]\n    n, d = np.shape(Xall)\n    times = 2\nelif data_name == 'pima':\n    '''Pima data'''\n    da_pima = pd.read_csv('../input/imbalance-person/Classification-Tree-with-Surface-to-Volume-ratio-Regularization-master/diabetes.csv')\n    da = da_pima.values\n    Xall = da[:,0:8]\n    Yall = da[:,8]\n    Yall = Yall.astype(int)\n    expan = 1.01\n    for i in range(8):\n        Xall[:,i] = Xall[:,i] / np.max(Xall[:,i]) / expan\n    n, d = np.shape(Xall)\n    times = 1\nelif data_name == 'wine':\n    '''Wine data'''\n    da_wine = pd.read_csv('../input/imbalance-person/Classification-Tree-with-Surface-to-Volume-ratio-Regularization-master/winequality-red.csv', sep=';')\n    da = da_wine.values\n    Xall = da[:,0:11]\n    Yall = np.zeros(np.shape(Xall)[0])\n    Yall[np.flatnonzero(da[:,11]>=7)] = 1\n    n, d = np.shape(Xall)\n    times = 5\nelif data_name == 'abalone':\n    '''Abalone data'''\n    da_abalone = pd.read_csv('../input/imbalance-person/Classification-Tree-with-Surface-to-Volume-ratio-Regularization-master/abalone.data', sep=',')\n    da_abalone = da_abalone.drop(columns='M')\n    da = da_abalone.values\n    Xall = da[:,0:7]\n    n1_ind = np.flatnonzero(da[:,7] == 18)\n    n0_ind = np.flatnonzero(da[:,7] == 9)\n    all_ind = np.concatenate((n1_ind, n0_ind))\n    Xall = da[all_ind, 0:7]\n    Yall = np.zeros(len(all_ind))\n    Yall[0:len(n1_ind)] = 1\n    n, d = np.shape(Xall)\n    times = 15\nelif data_name == 'satimage':\n    '''For Satimage data'''\n    da_a = pd.read_table('../input/imbalance-person/Classification-Tree-with-Surface-to-Volume-ratio-Regularization-master/sat_train.txt',\n                         sep=' ', header=None, error_bad_lines=False, keep_default_na=False)\n    da_a = da_a.values\n    Xa = da_a[:,0:36]\n    Ya = da_a[:,36]\n    Ya = np.int_(Ya==4)\n    da_b = pd.read_table('../input/imbalance-person/Classification-Tree-with-Surface-to-Volume-ratio-Regularization-master/sat_test.txt',\n                         sep=' ', header=None, error_bad_lines=False,keep_default_na=False)\n    da_b = da_b.values\n    Xb = da_b[:,0:36]\n    Yb = da_b[:,36]\n    Yb = np.int_(Yb==4)\n    Xall = np.vstack((Xa, Xb))\n    Yall = np.concatenate((Ya, Yb))\n    n, d = np.shape(Xall)\n    times = 8\nelif data_name == 'page':\n    da0 = pd.read_table('../input/imbalance-person/Classification-Tree-with-Surface-to-Volume-ratio-Regularization-master/page-blocks0.dat',\n                        sep=',', header=None)\n    da = da0.values\n    Xall = da[:,0:10]\n    n1_ind = np.flatnonzero(da[:,10] == ' positive')\n    Yall = np.zeros(np.shape(da)[0],dtype=int)\n    Yall[n1_ind] = 1\n    n, d = np.shape(Xall)\n    times = int((n-len(n1_ind))/len(n1_ind)) - 1\nelif data_name == 'yeast':\n    da0 = pd.read_table('../input/imbalance-person/Classification-Tree-with-Surface-to-Volume-ratio-Regularization-master/yeast4.dat',\n                        sep=',', header=None)\n    da = da0.values\n    n, d = np.shape(da)\n    d = d-1\n    Xall = da[:,0:d]\n    n1_ind = np.flatnonzero(da[:,d] == ' positive')\n    Yall = np.zeros(n,dtype=int)\n    Yall[n1_ind] = 1\n    times = int((n-len(n1_ind))/len(n1_ind)) - 1        \nelif data_name == 'segment':\n    da0 = pd.read_table('../input/imbalance-person/Classification-Tree-with-Surface-to-Volume-ratio-Regularization-master/segment0.dat', sep=',', header=None)\n    da = np.delete(da0.values, 2, axis=1)      ## remove the second feature as it contains no information\n    n, d = np.shape(da)\n    d = d-1\n    Xall = da[:,0:d]\n    n1_ind = np.flatnonzero(da[:,d] == ' positive')\n    Yall = np.zeros(n,dtype=int)\n    Yall[n1_ind] = 1\n    times = int((n-len(n1_ind))/len(n1_ind)) - 1        \nelif data_name == 'ecoli':\n    da0 = pd.read_table('../input/imbalance-person/Classification-Tree-with-Surface-to-Volume-ratio-Regularization-master/ecoli.dat', sep=',', header=None)\n    da = np.delete(da0.values, 3, axis=1)       ## remove the third feature as it contains little information \n    n, d = np.shape(da)\n    d = d-1\n    Xall = da[:,0:d]\n    n1_ind = np.flatnonzero(da[:,d] == ' pp')\n    Yall = np.zeros(n,dtype=int)\n    Yall[n1_ind] = 1\n    times = int((n-len(n1_ind))/len(n1_ind)) - 1           \nelif data_name == 'glass2':\n    da0 = pd.read_table('../input/imbalance-person/Classification-Tree-with-Surface-to-Volume-ratio-Regularization-master/glass2.dat', sep=',', header=None)\n    da = da0.values      \n    n, d = np.shape(da)\n    d = d-1\n    Xall = da[:,0:d]\n    n1_ind = np.flatnonzero(da[:,d] == ' positive')\n    Yall = np.zeros(n,dtype=int)\n    Yall[n1_ind] = 1\n    times = int((n-len(n1_ind))/len(n1_ind)) - 1     \nelif data_name == 'phoneme':\n    da0 = pd.read_table('../input/imbalance-person/Classification-Tree-with-Surface-to-Volume-ratio-Regularization-master/phoneme.dat', sep=',', header=None)\n    da = da0.values      \n    n, d = np.shape(da)\n    d = d-1\n    Xall = da[:,0:d]\n    n1_ind = np.flatnonzero(da[:,d])\n    Yall = np.zeros(n,dtype=int)\n    Yall[n1_ind] = 1\n    times = int((n-len(n1_ind))/len(n1_ind)) - 1           \nelif data_name == 'titanic':\n    da0 = pd.read_table('../input/imbalance-person/Classification-Tree-with-Surface-to-Volume-ratio-Regularization-master/titanic.dat', sep=',', header=None)\n    da = da0.values      \n    n, d = np.shape(da)\n    d = d-1\n    Xall = da[:,0:d]\n    n1_ind = np.flatnonzero(da[:,d]==1)\n    Yall = np.zeros(n,dtype=int)\n    Yall[n1_ind] = 1\n    times = int((n-len(n1_ind))/len(n1_ind)) - 1     \n\n    \n","metadata":{"execution":{"iopub.status.busy":"2022-01-04T06:04:50.037355Z","iopub.execute_input":"2022-01-04T06:04:50.037623Z","iopub.status.idle":"2022-01-04T06:04:50.094793Z","shell.execute_reply.started":"2022-01-04T06:04:50.037594Z","shell.execute_reply":"2022-01-04T06:04:50.094093Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"#%%\n'''Experiment Functions'''\ndef performance_stats(TP, FP, FN, TN):\n    TPR = TP / (TP+FN)\n    if TP+FP > 0:\n        precision = TP / (TP+FP)\n    else:\n        precision = 0\n    accuracy = (TP+TN) / (TP+FP+FN+TN)\n    TNR = TN / (TN+FP)\n    G_mean = np.sqrt(TPR*TNR)\n    if precision > 0:\n        F_measure = 2*TPR*precision / (TPR+precision)\n    else:\n        F_measure = 0\n    return np.array([accuracy, precision, TPR, TNR, F_measure, G_mean])\n\ndef divide(n, m):\n    '''Function to divide n samples into m roughly equal folds.'''\n    n_low = n // m\n    out = np.ones(m,dtype=int) * n_low\n    remain = n % m\n    out[0:remain] = out[0:remain] + 1\n    return out\n\ndef id_divide(ids, m):\n    '''Function to divide id_seq into m roughly equal folds.'''\n    n = len(ids)\n    n_in_folds = divide(n, m)\n    id_lst = [None]*m\n    loc = 0\n    for i in range(m):\n        loc_new = loc + n_in_folds[i]\n        id_lst[i] = ids[loc:loc_new]\n        loc = loc_new\n    return id_lst  \n\ndef experiment_runner(ids, Xall, Yall, n_cv, n_cv_out, pen_lst, weight, c0, alpha_lst):\n    X = Xall[ids,:]\n    Y = Yall[ids]\n    n = len(Y)\n    n1 = np.int_(np.sum(Y))\n    n0 = n - n1\n    n1_divide = divide(n1, n_cv_out)\n    n0_divide = divide(n0, n_cv_out)\n    id_1 = np.flatnonzero(Y)            ## all variables starting with \"id\" are indices of X and Y (not Xall or Yall)\n    id_0 = np.flatnonzero(Y==0)\n    TPFP_svr = np.zeros(2)\n    TPFP_svr_select = np.zeros(2)\n    TPFP_duplicate = np.zeros(2)\n    TPFP_SMOTE = np.zeros(2)\n    TPFP_BSMOTE = np.zeros(2)\n    TPFP_ADASYN = np.zeros(2)    \n    \n    for test_fold_no in range(n_cv_out):    \n        id_1test_start = np.int_(np.sum(n1_divide[0:test_fold_no]))\n        id_0test_start = np.int_(np.sum(n0_divide[0:test_fold_no]))\n        id_1test = id_1[id_1test_start:(id_1test_start+n1_divide[test_fold_no])]\n        id_1train = np.delete(id_1, np.arange(id_1test_start,id_1test_start+n1_divide[test_fold_no]))\n        id_0test = id_0[id_0test_start:(id_0test_start+n0_divide[test_fold_no])]\n        id_0train = np.delete(id_0, np.arange(id_0test_start,id_0test_start+n0_divide[test_fold_no]))\n        n1train = len(id_1train)\n        n1test = len(id_1test)\n        n0train = len(id_0train)\n        n0test = len(id_0test)\n        id_1train_cv = id_divide(id_1train, n_cv)\n        id_0train_cv = id_divide(id_0train, n_cv)\n        id_cv = [None] * n_cv\n        for k in range(n_cv):\n            id_cv[k] = np.concatenate((id_1train_cv[k], id_0train_cv[k]))\n        id_train = np.concatenate((id_1train, id_0train))\n        Xtrain = X[id_train,:]\n        Ytrain = Y[id_train]\n        id_test = np.concatenate((id_1test, id_0test))\n        Xtest = X[id_test,:]\n        Ytest = Y[id_test]\n        \n        '''SVR tree'''\n        F_lst = np.zeros(len(pen_lst))\n        for j in range(len(pen_lst)):\n            TP = 0\n            FP = 0\n            for k in range(n_cv):\n                id_cv_copy = id_cv.copy()\n                del id_cv_copy[k]\n                id_temp = np.concatenate(id_cv_copy)\n                Xtrain_temp = X[id_temp,:]\n                Ytrain_temp = Y[id_temp]\n                Xtest_temp = X[id_cv[k],:]\n                Ytest_temp = Y[id_cv[k]]\n                tr_svr = Tree.tree()\n                tr_svr.fit_sv(Xtrain_temp, Ytrain_temp, pen_lst[j], weight=weight, feature_select=False, maximal_leaves=2*np.sqrt(n*2/3))\n                Y_pred_temp = tr_svr.predict(Xtest_temp)\n                TP += np.sum(Y_pred_temp[np.flatnonzero(Ytest_temp)])\n                FP += np.sum(Y_pred_temp[np.flatnonzero(Ytest_temp==0)])\n            if TP > 0:\n                tpr = TP / n1train\n                precision = TP / (TP+FP)\n                F_lst[j] = 2*tpr*precision / (tpr+precision)\n        para_id = np.argmax(F_lst)\n        tr_svr = Tree.tree()\n        tr_svr.fit_sv(Xtrain, Ytrain, pen_lst[para_id], weight=weight, feature_select=False, maximal_leaves=2*np.sqrt(n*2/3))\n        Y_pred = tr_svr.predict(Xtest)    \n        TP = np.sum(Y_pred[np.flatnonzero(Ytest)])\n        FP = np.sum(Y_pred[np.flatnonzero(Ytest==0)])\n        TPFP_svr = TPFP_svr + np.array([TP, FP])\n            \n        '''SVR tree with feature selection'''\n        F_lst = np.zeros(len(pen_lst))\n        for j in range(len(pen_lst)):\n            TP = 0\n            FP = 0\n            for k in range(n_cv):\n                id_cv_copy = id_cv.copy()\n                del id_cv_copy[k]\n                id_temp = np.concatenate(id_cv_copy)\n                Xtrain_temp = X[id_temp,:]\n                Ytrain_temp = Y[id_temp]\n                Xtest_temp = X[id_cv[k],:]\n                Ytest_temp = Y[id_cv[k]]\n                tr_svr_select = Tree.tree()\n                tr_svr_select.fit_sv(Xtrain_temp, Ytrain_temp, pen_lst[j], weight=weight, feature_select=True, c0=c0, maximal_leaves=2*np.sqrt(n*2/3))\n                Y_pred_temp = tr_svr_select.predict(Xtest_temp)\n                TP += np.sum(Y_pred_temp[np.flatnonzero(Ytest_temp)])\n                FP += np.sum(Y_pred_temp[np.flatnonzero(Ytest_temp==0)])\n            if TP > 0:\n                tpr = TP / n1train\n                precision = TP / (TP+FP)\n                F_lst[j] = 2*tpr*precision / (tpr+precision)\n        para_id = np.argmax(F_lst)\n        tr_svr_select = Tree.tree()\n        tr_svr_select.fit_sv(Xtrain, Ytrain, pen_lst[para_id], weight=weight, feature_select=True, c0=c0, maximal_leaves=2*np.sqrt(n*2/3))\n        Y_pred = tr_svr_select.predict(Xtest)    \n        TP = np.sum(Y_pred[np.flatnonzero(Ytest)])\n        FP = np.sum(Y_pred[np.flatnonzero(Ytest==0)])\n        TPFP_svr_select = TPFP_svr_select + np.array([TP, FP])\n    \n        '''CART with duplicate samples'''\n        F_lst = np.zeros(len(alpha_lst))\n        TP_lst = np.zeros(len(alpha_lst))\n        FP_lst = np.zeros(len(alpha_lst))\n        for k in range(n_cv):\n            id_cv_copy = id_cv.copy()\n            del id_cv_copy[k]\n            id_temp = np.concatenate(id_cv_copy)\n            Xtrain_temp = X[id_temp,:]\n            Ytrain_temp = Y[id_temp]\n            Xtest_temp = X[id_cv[k],:]\n            Ytest_temp = Y[id_cv[k]]\n            sampler_now = sampler.sampler(times, 'duplicate')\n            Xtrain_res, Ytrain_res = sampler_now.fit_resample(Xtrain_temp, Ytrain_temp)\n            tr_cart = Tree.tree()\n            tr_cart.fit(Xtrain_res, Ytrain_res)\n            treelst, alpha_prune_lst, tot_leaf_lst = tr_cart.Prune()\n            alpha_prune_lst = np.append(alpha_prune_lst, np.inf)\n            for j in range(len(alpha_lst)):\n                i = 0\n                while alpha_prune_lst[i+1] <= alpha_lst[j]:\n                    i = i + 1\n                Y_pred_temp = treelst[i].predict(Xtest_temp)\n                TP_lst[j] += np.sum(Y_pred_temp[np.flatnonzero(Ytest_temp)])\n                FP_lst[j] += np.sum(Y_pred_temp[np.flatnonzero(Ytest_temp==0)])\n        for j in range(len(alpha_lst)):\n            if TP_lst[j] > 0:\n                tpr = TP_lst[j] / n1train\n                precision = TP_lst[j] / (TP_lst[j]+FP_lst[j])\n                F_lst[j] = 2*tpr*precision / (tpr+precision)   \n        para_id = np.argmax(F_lst)\n        tr_cart = Tree.tree()\n        tr_cart.fit(Xtrain, Ytrain)\n        treelst, alpha_prune_lst, tot_leaf_lst = tr_cart.Prune()\n        alpha_prune_lst = np.append(alpha_prune_lst, np.inf)\n        i = 0\n        while alpha_prune_lst[i+1] <= alpha_lst[para_id]:\n            i = i + 1\n        Y_pred = treelst[i].predict(Xtest)\n        TP = np.sum(Y_pred[np.flatnonzero(Ytest)])\n        FP = np.sum(Y_pred[np.flatnonzero(Ytest==0)])\n        TPFP_duplicate = TPFP_duplicate + np.array([TP, FP])\n        \n        '''CART with SMOTE'''\n        F_lst = np.zeros(len(alpha_lst))\n        TP_lst = np.zeros(len(alpha_lst))\n        FP_lst = np.zeros(len(alpha_lst))\n        for k in range(n_cv):\n            id_cv_copy = id_cv.copy()\n            del id_cv_copy[k]\n            id_temp = np.concatenate(id_cv_copy)\n            Xtrain_temp = X[id_temp,:]\n            Ytrain_temp = Y[id_temp]\n            Xtest_temp = X[id_cv[k],:]\n            Ytest_temp = Y[id_cv[k]]\n            sampler_now = sampler.sampler(times, 'SMOTE')\n            Xtrain_res, Ytrain_res = sampler_now.fit_resample(Xtrain_temp, Ytrain_temp)\n            tr_cart = Tree.tree()\n            tr_cart.fit(Xtrain_res, Ytrain_res)\n            treelst, alpha_prune_lst, tot_leaf_lst = tr_cart.Prune()\n            alpha_prune_lst = np.append(alpha_prune_lst, np.inf)\n            for j in range(len(alpha_lst)):\n                i = 0\n                while alpha_prune_lst[i+1] <= alpha_lst[j]:\n                    i = i + 1\n                Y_pred_temp = treelst[i].predict(Xtest_temp)\n                TP_lst[j] += np.sum(Y_pred_temp[np.flatnonzero(Ytest_temp)])\n                FP_lst[j] += np.sum(Y_pred_temp[np.flatnonzero(Ytest_temp==0)])\n        for j in range(len(alpha_lst)):\n            if TP_lst[j] > 0:\n                tpr = TP_lst[j] / n1train\n                precision = TP_lst[j] / (TP_lst[j]+FP_lst[j])\n                F_lst[j] = 2*tpr*precision / (tpr+precision)   \n        para_id = np.argmax(F_lst)\n        tr_cart = Tree.tree()\n        tr_cart.fit(Xtrain, Ytrain)\n        treelst, alpha_prune_lst, tot_leaf_lst = tr_cart.Prune()\n        alpha_prune_lst = np.append(alpha_prune_lst, np.inf)\n        i = 0\n        while alpha_prune_lst[i+1] <= alpha_lst[para_id]:\n            i = i + 1\n        Y_pred = treelst[i].predict(Xtest)\n        TP = np.sum(Y_pred[np.flatnonzero(Ytest)])\n        FP = np.sum(Y_pred[np.flatnonzero(Ytest==0)])\n        TPFP_SMOTE = TPFP_SMOTE + np.array([TP, FP])\n    \n        '''CART with B-SMOTE'''\n        F_lst = np.zeros(len(alpha_lst))\n        TP_lst = np.zeros(len(alpha_lst))\n        FP_lst = np.zeros(len(alpha_lst))\n        for k in range(n_cv):\n            id_cv_copy = id_cv.copy()\n            del id_cv_copy[k]\n            id_temp = np.concatenate(id_cv_copy)\n            Xtrain_temp = X[id_temp,:]\n            Ytrain_temp = Y[id_temp]\n            Xtest_temp = X[id_cv[k],:]\n            Ytest_temp = Y[id_cv[k]]\n            sampler_now = sampler.sampler(times, 'BSMOTE')\n            Xtrain_res, Ytrain_res = sampler_now.fit_resample(Xtrain_temp, Ytrain_temp)\n            tr_cart = Tree.tree()\n            tr_cart.fit(Xtrain_res, Ytrain_res)\n            treelst, alpha_prune_lst, tot_leaf_lst = tr_cart.Prune()\n            alpha_prune_lst = np.append(alpha_prune_lst, np.inf)\n            for j in range(len(alpha_lst)):\n                i = 0\n                while alpha_prune_lst[i+1] <= alpha_lst[j]:\n                    i = i + 1\n                Y_pred_temp = treelst[i].predict(Xtest_temp)\n                TP_lst[j] += np.sum(Y_pred_temp[np.flatnonzero(Ytest_temp)])\n                FP_lst[j] += np.sum(Y_pred_temp[np.flatnonzero(Ytest_temp==0)])\n        for j in range(len(alpha_lst)):\n            if TP_lst[j] > 0:\n                tpr = TP_lst[j] / n1train\n                precision = TP_lst[j] / (TP_lst[j]+FP_lst[j])\n                F_lst[j] = 2*tpr*precision / (tpr+precision)   \n        para_id = np.argmax(F_lst)\n        tr_cart = Tree.tree()\n        tr_cart.fit(Xtrain, Ytrain)\n        treelst, alpha_prune_lst, tot_leaf_lst = tr_cart.Prune()\n        alpha_prune_lst = np.append(alpha_prune_lst, np.inf)\n        i = 0\n        while alpha_prune_lst[i+1] <= alpha_lst[para_id]:\n            i = i + 1\n        Y_pred = treelst[i].predict(Xtest)\n        TP = np.sum(Y_pred[np.flatnonzero(Ytest)])\n        FP = np.sum(Y_pred[np.flatnonzero(Ytest==0)])\n        TPFP_BSMOTE = TPFP_BSMOTE + np.array([TP, FP])\n     \n        '''CART with ADASYN'''\n        F_lst = np.zeros(len(alpha_lst))\n        TP_lst = np.zeros(len(alpha_lst))\n        FP_lst = np.zeros(len(alpha_lst))\n        for k in range(n_cv):\n            id_cv_copy = id_cv.copy()\n            del id_cv_copy[k]\n            id_temp = np.concatenate(id_cv_copy)\n            Xtrain_temp = X[id_temp,:]\n            Ytrain_temp = Y[id_temp]\n            Xtest_temp = X[id_cv[k],:]\n            Ytest_temp = Y[id_cv[k]]\n            sampler_now = sampler.sampler(times, 'ADASYN')\n            Xtrain_res, Ytrain_res = sampler_now.fit_resample(Xtrain_temp, Ytrain_temp)\n            tr_cart = Tree.tree()\n            tr_cart.fit(Xtrain_res, Ytrain_res)\n            treelst, alpha_prune_lst, tot_leaf_lst = tr_cart.Prune()\n            alpha_prune_lst = np.append(alpha_prune_lst, np.inf)\n            for j in range(len(alpha_lst)):\n                i = 0\n                while alpha_prune_lst[i+1] <= alpha_lst[j]:\n                    i = i + 1\n                Y_pred_temp = treelst[i].predict(Xtest_temp)\n                TP_lst[j] += np.sum(Y_pred_temp[np.flatnonzero(Ytest_temp)])\n                FP_lst[j] += np.sum(Y_pred_temp[np.flatnonzero(Ytest_temp==0)])\n        for j in range(len(alpha_lst)):\n            if TP_lst[j] > 0:\n                tpr = TP_lst[j] / n1train\n                precision = TP_lst[j] / (TP_lst[j]+FP_lst[j])\n                F_lst[j] = 2*tpr*precision / (tpr+precision)   \n        para_id = np.argmax(F_lst)\n        tr_cart = Tree.tree()\n        tr_cart.fit(Xtrain, Ytrain)\n        treelst, alpha_prune_lst, tot_leaf_lst = tr_cart.Prune()\n        alpha_prune_lst = np.append(alpha_prune_lst, np.inf)\n        i = 0\n        while alpha_prune_lst[i+1] <= alpha_lst[para_id]:\n            i = i + 1\n        Y_pred = treelst[i].predict(Xtest)\n        TP = np.sum(Y_pred[np.flatnonzero(Ytest)])\n        FP = np.sum(Y_pred[np.flatnonzero(Ytest==0)])\n        TPFP_ADASYN = TPFP_ADASYN + np.array([TP, FP])\n    \n    results_svr = performance_stats(TPFP_svr[0], TPFP_svr[1], n1-TPFP_svr[0], n0-TPFP_svr[1])\n    results_svr_select = performance_stats(TPFP_svr_select[0], TPFP_svr_select[1], n1-TPFP_svr_select[0], n0-TPFP_svr_select[1])\n    results_duplicate = performance_stats(TPFP_duplicate[0], TPFP_duplicate[1], n1-TPFP_duplicate[0], n0-TPFP_duplicate[1])\n    results_SMOTE = performance_stats(TPFP_SMOTE[0], TPFP_SMOTE[1], n1-TPFP_SMOTE[0], n0-TPFP_SMOTE[1])\n    results_BSMOTE = performance_stats(TPFP_BSMOTE[0], TPFP_BSMOTE[1], n1-TPFP_BSMOTE[0], n0-TPFP_BSMOTE[1])\n    results_ADASYN = performance_stats(TPFP_ADASYN[0], TPFP_ADASYN[1], n1-TPFP_ADASYN[0], n0-TPFP_ADASYN[1])\n    \n    return (results_svr, results_svr_select, results_duplicate, results_SMOTE, results_BSMOTE, results_ADASYN)\n        \n\n\n","metadata":{"execution":{"iopub.status.busy":"2022-01-04T06:04:56.717951Z","iopub.execute_input":"2022-01-04T06:04:56.718324Z","iopub.status.idle":"2022-01-04T06:04:56.811685Z","shell.execute_reply.started":"2022-01-04T06:04:56.718277Z","shell.execute_reply":"2022-01-04T06:04:56.810615Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"#%%\n'''Linux exepriments'''\nseednum = 40\nnp.random.seed(seednum)\nnexps = 20     ## number of nested cross-validation experiments\nn1 = len(np.flatnonzero(Yall))\nn0 = n - n1\nid_permutes = np.zeros((nexps, n),dtype=int)\nfor i in range(nexps):\n    id_permutes[i,:] = np.random.permutation(n)\nid_filename = data_name+'_seed'+str(seednum)+'_ids'\nnp.save(id_filename, id_permutes) \nn_cv = 5\nn_cv_out = 3\ntrain_ratio = 1 - 1/n_cv_out\nn1train = np.int_(n1*train_ratio)\nn0train = np.int_(n0*train_ratio)\n \n## The below alpha_lst is for common datasets\nalpha_lst = np.array([0, 1/256, 1/128, 1/64, 1/32, 1/16, 0.125, 0.177, 0.25, 0.35, 0.5, 0.71, 1, 1.4, 2, 2.8, 4, 5.7, 8, 11, 16, 22, 32, 44, 64, 89, 128, 179, 256, 358, 512, 716, 1024, 1450, 2048, 2896, 4096]) * 10**(-3) * (n*train_ratio)**(-1/3)\n## The below alpha_lst is for satimage datasets\n# alpha_lst = np.array([0, 1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8192, 15000, 30000, 60000, 120000]) * 10**(-3) * (n0train+n1train)**(-1/3)\npen_lst = np.array([0, 1, 1.4, 2, 2.8, 4, 5.7, 8, 11, 16, 22, 32, 44, 64, 89, 128, 179, 256, 358, 512, 716, 1024]) * 10**(-3) * (n0train+n1train)**(-1/3)\nweight=times+1\nc0=4\n\ninputs = [None]*nexps\nprint('Run experiments for '+str(data_name))\nt1 = time.time()\nprint('head time: '+str(t1-t0))\nfor i in range(nexps):\n    inputs[i] = (id_permutes[i,:], Xall, Yall, n_cv, n_cv_out, pen_lst, weight, c0, alpha_lst)\nwith multip.Pool(processes=nexps) as pool:\n    outputs = pool.starmap(experiment_runner, inputs)    \nt2 = time.time()\nprint('main programs time: '+str(t2-t1))\noutputs_filename = data_name+'_seed'+str(seednum)+'_outputs'\nnp.save(outputs_filename, outputs)\n\nresults_mat_svr = np.zeros((nexps, 6))\nresults_mat_svr_select = np.zeros((nexps, 6))\nresults_mat_duplicate = np.zeros((nexps, 6))\nresults_mat_SMOTE = np.zeros((nexps, 6))\nresults_mat_BSMOTE = np.zeros((nexps, 6))\nresults_mat_ADASYN = np.zeros((nexps, 6))\nfor i in range(nexps):\n    output_now = outputs[i]\n    results_mat_svr[i,:] = output_now[0]\n    results_mat_svr_select[i,:] = output_now[1]\n    results_mat_duplicate[i,:] = output_now[2]\n    results_mat_SMOTE[i,:] = output_now[3]\n    results_mat_BSMOTE[i,:] = output_now[4]\n    results_mat_ADASYN[i,:] = output_now[5]\n\nmean_summary = np.zeros((6,6))\nstd_summary = np.zeros((6,6))\n'''The resulst, by orders, are: accuracy, precision, TPR, TNR, F_measure, G_mean'''\nmean_summary[0,:] = np.mean(results_mat_svr, axis=0)\nmean_summary[1,:] = np.mean(results_mat_svr_select, axis=0)\nmean_summary[2,:] = np.mean(results_mat_duplicate, axis=0)\nmean_summary[3,:] = np.mean(results_mat_SMOTE, axis=0)\nmean_summary[4,:] = np.mean(results_mat_BSMOTE, axis=0)\nmean_summary[5,:] = np.mean(results_mat_ADASYN, axis=0)\n\nstd_summary[0,:] = np.std(results_mat_svr, axis=0)\nstd_summary[1,:] = np.std(results_mat_svr_select, axis=0)\nstd_summary[2,:] = np.std(results_mat_duplicate, axis=0)\nstd_summary[3,:] = np.std(results_mat_SMOTE, axis=0)\nstd_summary[4,:] = np.std(results_mat_BSMOTE, axis=0)\nstd_summary[5,:] = np.std(results_mat_ADASYN, axis=0)\n\nt3 = time.time()\nprint('tail time: '+str(t3-t2))\n\nmean_filename = data_name+'_seed'+str(seednum)+'_mean'\nnp.save(mean_filename, mean_summary)\nstd_filename = data_name+'_seed'+str(seednum)+'_std'\nnp.save(std_filename, std_summary)\n\n''' Print the key performance measure stats and average number of features selected. '''\nprint(mean_summary)\nprint(std_summary)\n","metadata":{"execution":{"iopub.status.busy":"2022-01-04T06:05:04.422637Z","iopub.execute_input":"2022-01-04T06:05:04.422996Z","iopub.status.idle":"2022-01-04T07:37:00.567828Z","shell.execute_reply.started":"2022-01-04T06:05:04.422959Z","shell.execute_reply":"2022-01-04T07:37:00.566844Z"},"trusted":true},"execution_count":15,"outputs":[]}]}